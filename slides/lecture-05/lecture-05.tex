\documentclass{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{1, 0.894, 0.769}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.824,0.412,0.118}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{1,0.894,0.71}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.824,0.706,0.549}{#1}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.941,0.902,0.549}{#1}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.804,0.776,0.451}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.78,0.941,0.545}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{1,0.78,0.769}{#1}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{../371g-slides}
\title{Residuals and autocorrelation 1}
\subtitle{Lecture 5}
\author{STA 371G}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
  
  

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}
    \begin{frame}{Announcements}
      \begin{itemize}
        \item Homework 1 due Thursday at 11:59 PM, in MyStatLab
        \item Submit an R script in Canvas that contains the commands you used for each problem
      \end{itemize}
    \end{frame}

    \section{Transformations}

    \begin{frame}{Residuals}
      Recall that the \alert{residual} for the $i$th case in the data is $Y_i - \hat Y_i$.

      \begin{itemize}
        \item When the residual is \emph{positive}, the actual $Y$-value is \emph{higher} than our predicted $Y$-value.
        \item When the residual is \emph{negative}, the actual $Y$-value is \emph{lower} than our predicted $Y$-value.
      \end{itemize}

      \bigskip

      Looking at residuals can tell us a lot about how well a model is working, and give us ideas for how to improve it.
    \end{frame}

    \begin{frame}{Mileage efficiency data set}
      The data set \texttt{cars} contains specs for 392 different cars. We'll focus on two variables:
      \begin{itemize}
        \item \textbf{MPG} is fuel efficiency, measured as miles per gallon
        \item \textbf{Weight} is the weight of the car, in pounds
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{What problems do you see here?}
      \fontsm
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{plot}\hlstd{(MPG} \hlopt{~} \hlstd{Weight,} \hlkwc{data}\hlstd{=cars,} \hlkwc{pch}\hlstd{=}\hlnum{16}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"lightblue"}\hlstd{)}
\hlstd{> }\hlstd{model} \hlkwb{<-} \hlkwd{lm}\hlstd{(MPG} \hlopt{~} \hlstd{Weight,} \hlkwc{data}\hlstd{=cars)}
\hlstd{> }\hlkwd{abline}\hlstd{(model,} \hlkwc{col}\hlstd{=}\hlstr{"orange"}\hlstd{,} \hlkwc{lwd}\hlstd{=}\hlnum{4}\hlstd{)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-2-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{Using transformations to fix problems}
      \begin{itemize}
        \item Sometimes, a violation of regression assumptions can be fixed by transforming one or the other of the variables (or both).
        \item When we transform a variable, we have to also transform our interpretation of the equation.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{A bad example}
      \fontsm
      What if we predict MPG from squared weight?
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{cars}\hlopt{$}\hlstd{WeightSq} \hlkwb{<-} \hlstd{cars}\hlopt{$}\hlstd{Weight}\hlopt{^}\hlnum{2}
\hlstd{> }\hlkwd{plot}\hlstd{(MPG} \hlopt{~} \hlstd{WeightSq,} \hlkwc{data}\hlstd{=cars,} \hlkwc{pch}\hlstd{=}\hlnum{16}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"lightblue"}\hlstd{)}
\hlstd{> }\hlstd{sq.model} \hlkwb{<-} \hlkwd{lm}\hlstd{(MPG} \hlopt{~} \hlstd{WeightSq,} \hlkwc{data}\hlstd{=cars)}
\hlstd{> }\hlkwd{abline}\hlstd{(sq.model,} \hlkwc{col}\hlstd{=}\hlstr{"orange"}\hlstd{,} \hlkwc{lwd}\hlstd{=}\hlnum{4}\hlstd{)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-3-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{The log transformation}
      The \alert{log} transformation is frequently useful in regression, because many nonlinear relationships are naturally exponential.

      \bigskip

      \begin{itemize}
        \item $\log_b x = y$ when $b^y = x$.
        \item For example, $\log_{10} 1000 = 3$, $\log_{10} 100 = 2$, and $\log_{10} 10 = 1$.
        \item The natural log is $\log_e$, where $e \approx 2.72$ --- when we say ``log'' we will usually mean ``natural log.''
      \end{itemize}
    \end{frame}

    \begin{frame}
      \fullpagepicture{log}
    \end{frame}

    \begin{frame}[fragile]{Applying a log transformation}
      \fontsm
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{cars}\hlopt{$}\hlstd{LogWeight} \hlkwb{<-} \hlkwd{log}\hlstd{(cars}\hlopt{$}\hlstd{Weight)}
\hlstd{> }\hlkwd{plot}\hlstd{(MPG} \hlopt{~} \hlstd{LogWeight,} \hlkwc{data}\hlstd{=cars,} \hlkwc{pch}\hlstd{=}\hlnum{16}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"lightblue"}\hlstd{)}
\hlstd{> }\hlstd{log.model} \hlkwb{<-} \hlkwd{lm}\hlstd{(MPG} \hlopt{~} \hlstd{LogWeight,} \hlkwc{data}\hlstd{=cars)}
\hlstd{> }\hlkwd{abline}\hlstd{(log.model,} \hlkwc{col}\hlstd{=}\hlstr{"orange"}\hlstd{,} \hlkwc{lwd}\hlstd{=}\hlnum{4}\hlstd{)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-4-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{Checking assumptions of our new model}
      \fontsm
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{plot}\hlstd{(cars}\hlopt{$}\hlstd{LogWeight,} \hlkwd{residuals}\hlstd{(log.model),} \hlkwc{pch}\hlstd{=}\hlnum{16}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"pink"}\hlstd{)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-5-1.tex}

\end{knitrout}
      Linearity looks good, but homoscedasticity is still not satisfied!
    \end{frame}

    \begin{frame}[fragile]{Applying a second log transformation}
      \fontsm
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlstd{cars}\hlopt{$}\hlstd{LogMPG} \hlkwb{<-} \hlkwd{log}\hlstd{(cars}\hlopt{$}\hlstd{MPG)}
\hlstd{> }\hlkwd{plot}\hlstd{(LogMPG} \hlopt{~} \hlstd{LogWeight,} \hlkwc{data}\hlstd{=cars,} \hlkwc{pch}\hlstd{=}\hlnum{16}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"lightblue"}\hlstd{)}
\hlstd{> }\hlstd{log.log.model} \hlkwb{<-} \hlkwd{lm}\hlstd{(LogMPG} \hlopt{~} \hlstd{LogWeight,} \hlkwc{data}\hlstd{=cars)}
\hlstd{> }\hlkwd{abline}\hlstd{(log.log.model,} \hlkwc{col}\hlstd{=}\hlstr{"orange"}\hlstd{,} \hlkwc{lwd}\hlstd{=}\hlnum{4}\hlstd{)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-6-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{Checking assumptions of our new model}
      \fontsm
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{plot}\hlstd{(cars}\hlopt{$}\hlstd{LogWeight,} \hlkwd{residuals}\hlstd{(log.log.model),} \hlkwc{pch}\hlstd{=}\hlnum{16}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"pink"}\hlstd{)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-7-1.tex}

\end{knitrout}
      Much better---transforming MPG to log(MPG) gives us both linearity and homoscedasticity!
    \end{frame}

    \begin{frame}{Another example}
      Last class, we looked at predicting the \emph{returns} of AMZN based on the \emph{returns} of W5000. What if we just predicted the weekly closing \emph{price} of AMZN based on the price of W5000?

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}
\input{/tmp/figures/unnamed-chunk-8-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]
      \fontsm
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{verbatim}

Call:
lm(formula = AMZN ~ W5000, data = prices)

Residuals:
     Min       1Q   Median       3Q      Max 
-237.747 -100.865    2.552   72.583  260.783 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -510.293     48.869  -10.44   <2e-16 ***
W5000         57.333      2.996   19.14   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 120.4 on 259 degrees of freedom
Multiple R-squared:  0.5858,	Adjusted R-squared:  0.5842 
F-statistic: 366.2 on 1 and 259 DF,  p-value: < 2.2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}{Making a transformation to address the issues}
      \begin{center}
        The natural transformation of closing prices $\to$ returns address the issues with this model, as we saw last time---all assumptions are satisfied when using \% returns instead of absolute prices!

        \pause\bigskip
        \textbf{Key takeaway:} examine diagnostic plots of residuals to ensure regression assumptions are met; a high $R^2$ doesn't necessarily mean that model is appropriate!
      \end{center}
    \end{frame}

    \begin{frame}{Thinking about transformations}
      \begin{itemize}[<+->]
        \item Thinking about whether you want to stretch or squeeze one of the axes, and apply a transformation accordingly (e.g., $\sqrt x$ or $\log x$ to squeeze; $x^2$ or $e^x$ to stretch).
        \item Transformations of $Y$ can address both heteroscedasticity and nonlinearity; transformations of $X$ can only address nonlinearity.
        \item You might need to transform both $X$ and $Y$; if so, start by transforming $Y$ to address the heteroscedasticity, and then transform $X$ to address nonlinearity if necessary.
        \item It's OK to do a little trial and error!
      \end{itemize}
    \end{frame}

    \section{Extrapolation}

    \begin{frame}{Going beyond the data}
      \begin{itemize}
        \item It's natural to want to predict $Y$ beyond the $X$ values that we have in the data set (otherwise, why build a model in the first place?).
        \item But things get dicey when trying to predict $Y$ for an $X$ value that is far from the other $X$ values in the data set: how can we be so sure that the observed trend continues?
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]
      The shaded area shows the 95\% confidence interval for predicting the mean.
      As $X$ moves away from $\overline X$, the CI becomes wider since we know our estimates are less precise.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}
\input{/tmp/figures/unnamed-chunk-10-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}
      \begin{center}
        There's nothing wrong with extrapolating a little bit beyond the data, but when you move beyond the data even the confidence intervals may \emph{underestimate} the degree of uncertainty.
      \end{center}
    \end{frame}
  \end{darkframes}

\end{document}
