\documentclass{beamer}
\usepackage{../371g-slides}
% Uncomment these lines to print notes pages
% \pgfpagesuselayout{4 on 1}[letterpaper,border shrink=5mm,landscape]
% \setbeameroption{show only notes}
\title{Inference for simple regression 1}
\subtitle{Lecture 3}
\author{STA 371G}

\begin{document}
  <<setup, include=F, cache=F>>=
  opts_knit$set(global.par=T)
  knit_hooks$set(crop=hook_pdfcrop)
  opts_chunk$set(dev='tikz', external=F, fig.path='/tmp/figures/', comment=NA, fig.width=4, fig.height=3, crop=T, sanitize=T, tidy=F, prompt=T)
  knit_theme$set('camo')
  @
  <<include=F, cache=F>>=
  par(fg='#fefefe', col.axis='#fefefe', col.lab='#fefefe', col.main="#fefefe", mar=c(5.1, 4.1, 1.1, 2.1))
  @
  <<include=F>>=
  addhealth <- read.csv("../../data/addhealth.csv")
  age <- addhealth$h4to34
  age[age >= 96] <- NA
  num.drinks <- addhealth$h4to36
  num.drinks[num.drinks >= 96] <- NA

  options(digits=2)
  model <- lm(num.drinks ~ age)
  beta0 <- model$coefficients["(Intercept)"]
  beta1 <- model$coefficients['age']
  r.squared <- summary(model)$r.squared
  age.se <- summary(model)$coefficients[,"Std. Error"]["age"]
  @

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}
    \begin{frame}{Measuring goodness-of-fit}
      \begin{itemize}[<+->]
        \item $R^2$ measures the fraction of the variation in $Y$ explained by $X$;
        in our analysis from last time, $R^2=\Sexpr{r.squared}$.
        \item The \alert{standard error of the regression} $s_e$ can be roughly interpreted as
        the standard deviation of the residuals.
      \end{itemize}
    \end{frame}

    \begin{frame}{Interpreting the standard error of the regression $s_e = \Sexpr{summary(model)$sigma}$}
      \begin{itemize}[<+->]
        \item The \alert{residual} for the $i$th case is $Y_i - \hat Y_i$
        \item The residuals are approximately Normally distributed
        \item The mean of the residuals is 0 (why?)
        \item Therefore: 95\% of the residuals are roughly within $\pm 2 s_e$
        \item In other words, 95\% of the time I expect my prediction to be off by at most $\Sexpr{2*summary(model)$sigma}$
      \end{itemize}
    \end{frame}

    \begin{frame}
      In our regression, $R^2=\Sexpr{r.squared}$.

      Is this ``significant?''
      \pause
      \begin{itemize}[<+->]
        \item \textbf{Statistical significance:} Can we reject the null hypothesis that the correlation between $X$ and $Y$ in the \emph{population} is zero?
        \item \textbf{Practical significance:} Is the relationship in our sample strong enough to be meaningful?
      \end{itemize}
    \end{frame}

    \begin{frame}{The overall null hypothesis for a regression model}
      The following are equivalent ways to express the overall null hypothesis:
      \begin{itemize}[<+->]
        \item $R^2=0$ (in the population)
        \item $\text{cor}(X,Y)=0$ (in the population)
        \item $\beta_1=0$
        \item The model has no predictive power
        \item Predictions from this model are no better than predicting $\overline Y$ for every case
      \end{itemize}
    \end{frame}

    \begin{frame}{Two ways to test the overall null hypothesis}
      \begin{itemize}
        \item The $F$-test (tests $H_0:R^2=0$ in the population)
        \item The $t$-test for the \emph{slope} ($\beta_1$) coefficient (tests $H_0:\beta_1=0$)
      \end{itemize}
      \pause
      Both of these methods are equivalent; the $p$-values will be exactly the same!
    \end{frame}

    \begin{frame}[fragile]
      \note{Point out where these $p$-values are found on the R output.}
      \fontsize{9}{9}\selectfont
      <<>>=
      model <- lm(num.drinks ~ age)
      summary(model)
      @
    \end{frame}

    \begin{frame}{What is our conclusion about $\beta_1$?}
      \begin{itemize}[<+->]
        \item There is a \textbf{statistically significant} relationship between the age someone starts drinking and how much they drink as an adult.
        \item Or: People that start drinking earlier in life consume \textbf{significantly more} alcohol when they drink as adults.
        \item Each additional year you wait to start drinking is associated with consuming \Sexpr{abs(beta1)} fewer drinks as an adult.
        \item Is this relationship \textbf{practically significant}?
      \end{itemize}
    \end{frame}

    \begin{frame}{Practical significance}
      \begin{itemize}
        \item To assess \textbf{statistical significance}, we look at the $p$-value
        \item To assess \textbf{practical significance}:
          \begin{itemize}
            \item We only consider it if we already have statistical significance (why?)
            \item Look at $R^2$, the standard error of the regression, and the magnitude of the coefficients
            \item It's ultimately a judgement call!
          \end{itemize}
      \end{itemize}
    \end{frame}

    \begin{frame}{Put a confidence interval on it}
      \begin{itemize}[<+->]
        \item Our best estimate for the \emph{effect} of a year's postponement of drinking is \Sexpr{abs(beta1)} fewer drinks as an adult
        \item We can use a confidence interval to give a range of plausible values for what this effect size is in the population
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Put a confidence interval on it}
      A confidence interval is always of the form \[ \text{estimate} \pm (\text{critical value})(\text{standard error}). \]
      \pause
      Recall that the critical value for a 95\% confidence interval is the cutoff value that cuts off 95\% of the area in the middle of the distribution; the sampling distribution of $\hat\beta_1$ is a $t$-distribution.

      <<echo=F>>=
        options(digits=7)
      @
      <<>>=
      n <- nobs(model)
      qt(0.975, n-2)
      @
    \end{frame}

    \begin{frame}[fragile]{Put a confidence interval on it}
      R will also calculate confidence intervals for us:
      <<>>=
      confint(model)
      @
      <<echo=F>>=
      options(digits=2)
      @
      \pause
      In other words, we are 95\% confident that the effect of each additional year's delay in starting to drink is between \Sexpr{abs(confint(model)["age","97.5 %"])} and \Sexpr{abs(confint(model)["age","2.5 %"])}.
    \end{frame}

    \begin{frame}{Put a confidence interval on it, part 2}
      We can also put a confidence interval on a prediction!

      Two kinds of intervals:
      \bigskip

      \begin{tabular}{lp{1in}p{2in}}
        \textbf{Confidence} & Predicting the mean value of $Y$ for a particular $X$. & Among all people that start drinking at age 21, how many drinks do have on average as adults? \\
        \textbf{Prediction} & Predicting $Y$ for a single new case. & If Bob started drinking at age 21, how many drinks do we think will have as an adult? \\
      \end{tabular}
    \end{frame}

    \begin{frame}[fragile]{Put a confidence interval on it, part 2}
      <<echo=F>>=
      options(digits=7)
      @
      <<>>=
      predict(model, list(age=21),
        interval='confidence')
      predict(model, list(age=21),
        interval='prediction')
      @

      \pause
      Why is the prediction interval wider?
      \lc
    \end{frame}

    \begin{frame}
      \fullpagepicture{xkcd}
    \end{frame}

    \begin{frame}{Correlation $\neq$ causation}
      Because the $p$-value is small, we can be highly confident that there is a relationship in
      the population between age of first drink and number of drinks consumed as an adult.

      \bigskip
      This could be because:

      \pause

      \begin{itemize}[<+->]
        \item Starting to drink earlier causes you to drink more as an adult.
        \item Being predisposed to drink more will cause you to start drinking sooner.
        \item There is a third variable that causes both early drinking and drinking more as an adult.
      \end{itemize}

      \pause
      We can't tell just by looking at this data set!
    \end{frame}

  \end{darkframes}

\end{document}
