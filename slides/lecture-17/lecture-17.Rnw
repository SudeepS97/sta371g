\documentclass{beamer}
\usepackage{preview}
\usepackage{../371g-slides}
\title{Logistic regression 2}
\subtitle{Lecture 17}
\author{STA 371G}

\begin{document}
  <<setup, include=F, cache=F>>=
  opts_knit$set(global.par=T)
  knit_hooks$set(crop=hook_pdfcrop)
  opts_chunk$set(dev='tikz', external=F, fig.path='/tmp/figures/', comment=NA, fig.width=4, fig.height=3, crop=T, sanitize=T, tidy=F)
  knit_theme$set('camo')
  library(okcupiddata)
  source("../../lecture-scripts/empirical-logit-plot.R")
  @
  <<include=F, cache=F>>=
  par(fg='#fefefe', col.axis='#fefefe', col.lab='#fefefe', col.main="#fefefe", mar=c(5.1, 4.1, 1.1, 2.1))
  @

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}
    \begin{frame}{Team evaluations for Project Part 2}
      \begin{itemize}
        \item If you forgot to submit your team evaluation for Project Part 2, you can still submit it anytime today, up until 11:59 PM!
      \end{itemize}
    \end{frame}

    \begin{frame}{Midterm 2!}
      \begin{itemize}[<+->]
        \item Midterm 2 will be held in class on Wednesday, April 10.
        \item It covers material through Lecture 18 (Wednesday, April 3).
        \item You can bring \alert{two} pages of notes with you (front and back).
        \item The TAs will hold an optional review session in \alert{GSB 3.130} on Sunday, April 7 at 6 PM.
        \item Don't forget about the weekly TA help sessions on Tuesday nights at 6 PM (also in GSB 3.130)!
      \end{itemize}
    \end{frame}

    \begin{frame}{Last time}
      <<echo=F>>=
      profiles$male <- ifelse(profiles$sex == "m", 1, 0)
      my.profiles <- subset(profiles, height >= 55 & height <= 80)
      model <- glm(male ~ height, data=my.profiles, family=binomial)
      b0 <- coef(model)['(Intercept)']
      b1 <- coef(model)['height']
      options(digits=2)
      @
      \begin{itemize}
        \item The OkCupid data set contains information about \Sexpr{nrow(profiles)} profiles from users of the OkCupid online dating service.
        \item We predicted sex (as a binary categorical variable) from height using logistic regression, and came up with the prediction equation:
        \[
          \log\text{odds} = \log\left(\frac{P(\text{male})}{1-P(\text{male})}\right) = \Sexpr{b0} + \Sexpr{b1}\cdot\text{height}.
        \]
        or, solving for $P(\text{male})$,
        \[
          \widehat{P(\text{male})} = \frac{e^{\Sexpr{b0} + \Sexpr{b1}\cdot\text{height}}}{1 + e^{\Sexpr{b0} + \Sexpr{b1}\cdot\text{height}}}
        \]
      \end{itemize}
    \end{frame}

    \section{Hypothesis testing}

    \begin{frame}{Testing the null hypothesis}
      \begin{center}
        As in regular linear regression, the overall null hypothesis is that $\beta_1=0$; we can test this by using the $p$-value for that variable on the output.

        \bigskip\pause
        Since $p$ is very small, we can reject the null hypothesis that $\beta_1=0$; i.e., there is a statistically significant relationship between height and sex.
      \end{center}
    \end{frame}

    \section{Evaluating the model}

    \begin{frame}{How good is our model?}
      \begin{itemize}[<+->]
        \item Unfortunately, the typical $R^2$ metric isn't available for logistic regression.
        \item However, there are many ``pseudo-$R^2$'' metrics that indicate model fit.
        \item But: most of these pseudo-$R^2$ metrics are difficult to interpret, so we'll focus on something simpler to interpret and communicate.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{How many cases did we accurately predict?}
      We could use our model to make a prediction of sex based on the probability. Suppose we say that our prediction is:
      \[
        \text{Prediction} = \begin{cases}
          \text{male}, & \text{if $\widehat{P(\text{male})} \geq 0.5$}, \\
          \text{female}, & \text{if $\widehat{P(\text{male})} < 0.5$}. \\
        \end{cases}
      \]

      \pause
      Now we can compute the fraction of people whose sex we correctly predicted:

      \fontsize{9}{9}\selectfont
      <<>>=
      predicted.male <- (predict(model, type="response") >= 0.5)
      actual.male <- (my.profiles$male == 1)
      sum(predicted.male == actual.male) / nrow(my.profiles)
      @
    \end{frame}

    \begin{frame}[fragile]{How many cases did we accurately predict?}
      \Sexpr{round(100*(sum(predicted.male == actual.male) / nrow(my.profiles)), 0)}\% sounds pretty good---what should we compare it against?

      \bigskip
      \pause

      We should compare \Sexpr{round(100*(sum(predicted.male == actual.male) / nrow(my.profiles)), 0)}\% against what we would have gotten if we just predicted the most common outcome (male) for everyone, without using any other information:
      \pause

      <<>>=
      sum(actual.male) / nrow(my.profiles)
      @

      \pause

      In other words, our model provided a ``lift'' in accuracy from \Sexpr{round(100*(sum(actual.male) / nrow(my.profiles)), 0)}\% to \Sexpr{round(100*(sum(predicted.male == actual.male) / nrow(my.profiles)), 0)}\%.
    \end{frame}

    \begin{frame}{The confusion matrix}
      Sometimes it is useful to understand what kinds of errors our model is making:
      \begin{itemize}
        \item \alert{True positives}: predicting male for someone that is male
        \item \alert{True negatives}: predicting female for someone that is female
        \item \alert{False positives}: predicting male for someone that is female
        \item \alert{False negatives}: predicting female for someone that is male
      \end{itemize}
      (If we had designated female as 1 and male as 0, these would have switched!)
    \end{frame}

    \begin{frame}[fragile]{The confusion matrix}
      <<>>=
      table(predicted.male, actual.male)
      prop.table(table(predicted.male, actual.male), 2)
      @
    \end{frame}

    \section{Checking assumptions}

    \begin{frame}{Checking assumptions}
      \begin{itemize}
        \item Independence
        \item Linearity
        \item Normality of residuals \redx
        \item Homoscedasticity / Equal variance \redx
      \end{itemize}
      With logistic regression, we don't need to check the last two assumptions (since $Y$ is binary).
    \end{frame}

    \begin{frame}{Checking assumptions: Independence}
      Like with linear regression, we check independence by thinking about the data conceptually: are the predictions the model makes likely to be independent from each other?
      \bigskip\pause

      \greencheckmark \alert{Yes!} Each case is a completely different person whose heights and genders are unrelated.
    \end{frame}

    \begin{frame}{Checking assumptions: Linearity}
      Look at the logistic regression model:
      \[
        \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X + \epsilon
      \]
      We need an approximately linear relationship between the \alert{log odds of success} and $X$, or, equivalently, a linear relationship between the log odds of success and what is predicted from our linear model on the right side of the equation.
    \end{frame}

    \begin{frame}[fragile]{Checking assumptions: Linearity}
      <<include=F>>=
      breaks <- 10
      percentiles <- quantile(predict(model), probs=seq(0, 1, 1/breaks))
      data <- data.frame(predicted.logits=predict(model),
                         y=model$model[,1],
                         groups=cut(predict(model), breaks=percentiles, labels=1:breaks))
      # Within each group, calculate actual number of successes and number of cases
      num.success <- table(data$y, data$groups)["1",]
      num.cases <- as.numeric(table(data$groups))
      # Calculate empirical probability within each group, with a small
      # adjustment to avoid infinity when p = 1
      empirical.probs <- (num.success + 0.5) / (num.cases + 1)
      # Calculate actual log odds within each group
      logits <- log(empirical.probs / (1 - empirical.probs))
      options(width=50)
      @

      To do this, we segment the predicted log odds into groups by deciles (bottom 10\%, next 10\%, up until the highest 10\%):

      <<>>=
      quantile(predict(model), probs=seq(0, 1, 0.1))
      @
    \end{frame}

    \begin{frame}{Checking assumptions: linearity}
      Then we'll calculate the empirical log odds within each group:

      \begin{center}
        \begin{tabular}{lllll}
          Predicted log odds & \# males & Total & $p=P(\text{male})$ & Log odds \\
          \hline
          $[ \Sexpr{percentiles[1]}, \Sexpr{percentiles[2]} ]$ & $\Sexpr{num.success[1]}$ &  $\Sexpr{num.cases[1]}$ & $\Sexpr{empirical.probs[1]}$ & $\Sexpr{logits[1]}$ \\
          $[ \Sexpr{percentiles[2]}, \Sexpr{percentiles[3]} ]$ & $\Sexpr{num.success[2]}$ &  $\Sexpr{num.cases[2]}$ & $\Sexpr{empirical.probs[2]}$ & $\Sexpr{logits[2]}$ \\
          $[ \Sexpr{percentiles[3]}, \Sexpr{percentiles[4]} ]$ & $\Sexpr{num.success[3]}$ &  $\Sexpr{num.cases[3]}$ & $\Sexpr{empirical.probs[3]}$ & $\Sexpr{logits[3]}$ \\
          $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
          $[ \Sexpr{percentiles[10]}, \Sexpr{percentiles[11]} ]$ & $\Sexpr{num.success[10]}$ &  $\Sexpr{num.cases[10]}$ & $\Sexpr{empirical.probs[10]}$ & $\Sexpr{logits[10]}$ \\

        \end{tabular}
      \end{center}

      Then we'll plot the empirical log odds against the mean of each decile; we'd like to see approximately the line $y=x$; this is called an \alert{empirical logit plot}.
    \end{frame}

    \begin{frame}[fragile]{Checking assumptions: Linearity}
      <<fig.height=2>>=
      empirical.logit.plot(model)
      @
      \pause\vspace{-0.5cm}
      \greencheckmark \alert{Yes!} This is approximately along the line $y=x$.
    \end{frame}
  \end{darkframes}
\end{document}
