\documentclass{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{1, 0.894, 0.769}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.824,0.412,0.118}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{1,0.894,0.71}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.824,0.706,0.549}{#1}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.941,0.902,0.549}{#1}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.804,0.776,0.451}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.78,0.941,0.545}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{1,0.78,0.769}{#1}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{preview}
\usepackage{../371g-slides}
\title{Logistic regression 2}
\subtitle{Lecture 17}
\author{STA 371G}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
  
  

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}
    \begin{frame}{Team evaluations for Project Part 2}
      \begin{itemize}
        \item If you forgot to submit your team evaluation for Project Part 2, you can still submit it anytime today, up until 11:59 PM!
      \end{itemize}
    \end{frame}

    \begin{frame}{Midterm 2!}
      \begin{itemize}[<+->]
        \item Midterm 2 will be held in class on Wednesday, April 10.
        \item It covers material through Lecture 18 (Wednesday, April 3).
        \item You can bring \alert{two} pages of notes with you (front and back).
        \item The TAs will hold an optional review session in \alert{GSB 3.130} on Sunday, April 7 at 6 PM.
        \item Don't forget about the weekly TA help sessions on Tuesday nights at 6 PM (also in GSB 3.130)!
      \end{itemize}
    \end{frame}

    \begin{frame}{Last time}

      \begin{itemize}
        \item The OkCupid data set contains information about 59946 profiles from users of the OkCupid online dating service.
        \item We predicted sex (as a binary categorical variable) from height using logistic regression, and came up with the prediction equation:
        \[
          \log\text{odds} = \log\left(\frac{P(\text{male})}{1-P(\text{male})}\right) = -44.45 + 0.66\cdot\text{height}.
        \]
        or, solving for $P(\text{male})$,
        \[
          \widehat{P(\text{male})} = \frac{e^{-44.45 + 0.66\cdot\text{height}}}{1 + e^{-44.45 + 0.66\cdot\text{height}}}
        \]
      \end{itemize}
    \end{frame}

    \section{Hypothesis testing}

    \begin{frame}{Testing the null hypothesis}
      \begin{center}
        As in regular linear regression, the overall null hypothesis is that $\beta_1=0$; we can test this by using the $p$-value for that variable on the output.

        \bigskip\pause
        Since $p$ is very small, we can reject the null hypothesis that $\beta_1=0$; i.e., there is a statistically significant relationship between height and sex.
      \end{center}
    \end{frame}

    \section{Evaluating the model}

    \begin{frame}{How good is our model?}
      \begin{itemize}[<+->]
        \item Unfortunately, the typical $R^2$ metric isn't available for logistic regression.
        \item However, there are many ``pseudo-$R^2$'' metrics that indicate model fit.
        \item But: most of these pseudo-$R^2$ metrics are difficult to interpret, so we'll focus on something simpler to interpret and communicate.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{How many cases did we accurately predict?}
      We could use our model to make a prediction of sex based on the probability. Suppose we say that our prediction is:
      \[
        \text{Prediction} = \begin{cases}
          \text{male}, & \text{if $\widehat{P(\text{male})} \geq 0.5$}, \\
          \text{female}, & \text{if $\widehat{P(\text{male})} < 0.5$}. \\
        \end{cases}
      \]

      \pause
      Now we can compute the fraction of people whose sex we correctly predicted:

      \fontsize{9}{9}\selectfont
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{predicted.male} \hlkwb{<-} \hlstd{(}\hlkwd{predict}\hlstd{(model,} \hlkwc{type}\hlstd{=}\hlstr{"response"}\hlstd{)} \hlopt{>=} \hlnum{0.5}\hlstd{)}
\hlstd{actual.male} \hlkwb{<-} \hlstd{(my.profiles}\hlopt{$}\hlstd{male} \hlopt{==} \hlnum{1}\hlstd{)}
\hlkwd{sum}\hlstd{(predicted.male} \hlopt{==} \hlstd{actual.male)} \hlopt{/} \hlkwd{nrow}\hlstd{(my.profiles)}
\end{alltt}
\begin{verbatim}
[1] 0.83
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{How many cases did we accurately predict?}
      83\% sounds pretty good---what should we compare it against?

      \bigskip
      \pause

      We should compare 83\% against what we would have gotten if we just predicted the most common outcome (male) for everyone, without using any other information:
      \pause

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlkwd{sum}\hlstd{(actual.male)} \hlopt{/} \hlkwd{nrow}\hlstd{(my.profiles)}
\end{alltt}
\begin{verbatim}
[1] 0.6
\end{verbatim}
\end{kframe}
\end{knitrout}

      \pause

      In other words, our model provided a ``lift'' in accuracy from 60\% to 83\%.
    \end{frame}

    \begin{frame}{The confusion matrix}
      Sometimes it is useful to understand what kinds of errors our model is making:
      \begin{itemize}
        \item \alert{True positives}: predicting male for someone that is male
        \item \alert{True negatives}: predicting female for someone that is female
        \item \alert{False positives}: predicting male for someone that is female
        \item \alert{False negatives}: predicting female for someone that is male
      \end{itemize}
      (If we had designated female as 1 and male as 0, these would have switched!)
    \end{frame}

    \begin{frame}[fragile]{The confusion matrix}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlkwd{table}\hlstd{(predicted.male, actual.male)}
\end{alltt}
\begin{verbatim}
              actual.male
predicted.male FALSE  TRUE
         FALSE 19466  5494
         TRUE   4623 30243
\end{verbatim}
\begin{alltt}
\hlkwd{prop.table}\hlstd{(}\hlkwd{table}\hlstd{(predicted.male, actual.male),} \hlnum{2}\hlstd{)}
\end{alltt}
\begin{verbatim}
              actual.male
predicted.male FALSE TRUE
         FALSE  0.81 0.15
         TRUE   0.19 0.85
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \section{Checking assumptions}

    \begin{frame}{Checking assumptions}
      \begin{itemize}
        \item Independence
        \item Linearity
        \item Normality of residuals \redx
        \item Homoscedasticity / Equal variance \redx
      \end{itemize}
      With logistic regression, we don't need to check the last two assumptions (since $Y$ is binary).
    \end{frame}

    \begin{frame}{Checking assumptions: Independence}
      Like with linear regression, we check independence by thinking about the data conceptually: are the predictions the model makes likely to be independent from each other?
      \bigskip\pause

      \greencheckmark \alert{Yes!} Each case is a completely different person whose heights and genders are unrelated.
    \end{frame}

    \begin{frame}{Checking assumptions: Linearity}
      Look at the logistic regression model:
      \[
        \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X + \epsilon
      \]
      We need an approximately linear relationship between the \alert{log odds of success} and $X$, or, equivalently, a linear relationship between the log odds of success and what is predicted from our linear model on the right side of the equation.
    \end{frame}

    \begin{frame}[fragile]{Checking assumptions: Linearity}
      

      To do this, we segment the predicted log odds into groups by deciles (bottom 10\%, next 10\%, up until the highest 10\%):

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlkwd{quantile}\hlstd{(}\hlkwd{predict}\hlstd{(model),} \hlkwc{probs}\hlstd{=}\hlkwd{seq}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{0.1}\hlstd{))}
\end{alltt}
\begin{verbatim}
   0%   10%   20%   30%   40%   50%   60%   70% 
-8.04 -2.75 -1.42 -0.76 -0.10  0.56  1.88  2.55 
  80%   90%  100% 
 3.21  3.87  8.50 
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}{Checking assumptions: linearity}
      Then we'll calculate the empirical log odds within each group:

      \begin{center}
        \begin{tabular}{lllll}
          Predicted log odds & \# males & Total & $p=P(\text{male})$ & Log odds \\
          \hline
          $[ -8.04, -2.75 ]$ & $256$ &  $7182$ & $0.04$ & $-3.3$ \\
          $[ -2.75, -1.42 ]$ & $1090$ &  $7659$ & $0.14$ & $-1.8$ \\
          $[ -1.42, -0.76 ]$ & $1579$ &  $4759$ & $0.33$ & $-0.7$ \\
          $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
          $[ 3.87, 8.5 ]$ & $5168$ &  $5208$ & $0.99$ & $4.85$ \\

        \end{tabular}
      \end{center}

      Then we'll plot the empirical log odds against the mean of each decile; we'd like to see approximately the line $y=x$; this is called an \alert{empirical logit plot}.
    \end{frame}

    \begin{frame}[fragile]{Checking assumptions: Linearity}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlkwd{empirical.logit.plot}\hlstd{(model)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-8-1.tex}

\end{knitrout}
      \pause\vspace{-0.5cm}
      \greencheckmark \alert{Yes!} This is approximately along the line $y=x$.
    \end{frame}
  \end{darkframes}
\end{document}
