\documentclass{beamer}
\usepackage{preview}
\usepackage{../371g-slides}
\title{Logistic regression 3}
\subtitle{Lecture 18}
\author{STA 371G}

\begin{document}
  <<setup, include=F, cache=F>>=
  opts_knit$set(global.par=T)
  knit_hooks$set(crop=hook_pdfcrop)
  opts_chunk$set(dev='tikz', external=F, fig.path='/tmp/figures/', comment=NA, fig.width=4, fig.height=3, crop=T, sanitize=T, tidy=F)
  knit_theme$set('camo')

  # https://stackoverflow.com/questions/48286722/rmarkdown-how-to-show-partial-output-from-chunk
  hook_output <- knit_hooks$get('output')
  knit_hooks$set(output = function(x, options) {
    if (!is.null(n <- options$out.lines)) {
      n <- as.numeric(n)
      x <- unlist(stringr::str_split(x, "\n"))
      nx <- length(x)
      x <- x[pmin(n,nx)]
      if(min(n) > 1)
        x <- c("...", x)
      if(max(n) < nx)
        x <- c(x, "...")
      x <- paste(c(x, "\n"), collapse = "\n")
    }
    hook_output(x, options)
  })

  library(okcupiddata)
  source("../../lecture-scripts/empirical-logit-plot.R")
  profiles$male <- ifelse(profiles$sex == "m", 1, 0)
  my.profiles <- subset(profiles, height >= 55 & height <= 80)
  @
  <<include=F, cache=F>>=
  par(fg='#fefefe', col.axis='#fefefe', col.lab='#fefefe', col.main="#fefefe", mar=c(5.1, 4.1, 1.1, 2.1))
  @

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}
    \begin{frame}{Reminder}
      \begin{itemize}
        \item Remember that Part 3 of the project is due on Friday at 11:59 PM!
      \end{itemize}
    \end{frame}

    \begin{frame}<beamer>
      \tableofcontents
    \end{frame}

    \section{Logistic regression with 2+ predictors}

    \begin{frame}{Adding another predictor}
      \begin{itemize}
        \item Just like with a linear regression model, we can add additional predictors to the model.
        \item Our interpretation of the coefficients in multiple logistic regression is similar to multiple linear regression, in the sense that each coefficient represents the predicted effect of one $X$ on $Y$, holding the other $X$ variables constant.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Adding another predictor}
      Let's add sexual orientation as a second predictor of gender, in addition to height:
      <<include=F>>=
      options(width=80, digits=2)
      @
      <<>>=
      model2 <- glm(male ~ height + orientation,
        data=my.profiles, family=binomial)
      @
      The \texttt{orientation} variable has three categories:
      <<>>=
      table(my.profiles$orientation)
      @
    \end{frame}

    \begin{frame}[fragile]
      \fontvsm\vspace{-0.3cm}
      <<echo=F>>=
      summary(model2)
      @
    \end{frame}

    \begin{frame}{Interpreting coefficients}
      Our prediction equation is:
      <<include=F>>=
      b0 <- coef(model2)['(Intercept)']
      b1 <- coef(model2)['height']
      b2 <- coef(model2)['orientationgay']
      b3 <- coef(model2)['orientationstraight']
      pct <- function(x) { paste(round(100 * x, 0), "\\%", sep="") }
      @
      \[
        \log\left(\frac{p}{1-p}\right) =
          \Sexpr{b0} +
          \Sexpr{b1}\cdot\text{height} +
          \Sexpr{b2}\cdot\text{gay} +
          \Sexpr{b3}\cdot\text{straight}.
      \]
      This means that:
      \begin{itemize}[<+->]
        \item Our predicted log odds of being male for someone who is bisexual and has a height of 0" is $\Sexpr{b0}$ (the intercept).
        \item Among people with the same sexual orientation, each additional inch of height corresponds to an increase in \Sexpr{pct(exp(b1)-1)} in predicted odds of being male (i.e., multiplied by $e^{\Sexpr{b1}} = \Sexpr{exp(b1)}$).
      \end{itemize}
    \end{frame}

    \begin{frame}{Interpreting coefficients}
      \[
        \log\left(\frac{p}{1-p}\right) =
          \Sexpr{b0} +
          \Sexpr{b1}\cdot\text{height} +
          \Sexpr{b2}\cdot\text{gay} +
          \Sexpr{b3}\cdot\text{straight}.
      \]
      \begin{itemize}[<+->]
        \item Among people of the same height, being gay increases the predicted odds of being male by \Sexpr{pct(exp(b2)-1)} (i.e., multiplied by $e^{\Sexpr{b2}} = \Sexpr{exp(b2)}$) compared to being bisexual.
        \item Among people of the same height, being straight increases the predicted odds of being male by \Sexpr{pct(exp(b3)-1)} (i.e., multiplied by $e^{\Sexpr{b3}} = \Sexpr{exp(b3)}$) compared to being bisexual.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Understanding what's going on}
      \fontsm
      <<>>=
      crosstabs <- table(my.profiles$sex, my.profiles$orientation)
      crosstabs
      @
    \end{frame}

    \begin{frame}[fragile]
      \fontsm
      <<>>=
      barplot(prop.table(crosstabs, 2), col=c("pink", "lightblue"),
        legend=T)
      @
    \end{frame}

    \begin{frame}{Converting back to probabilities}
      Because there is a nonlinear relationship between probability and odds, a particular percentage increase in odds does not correspond to a fixed change in probability. But it can be useful sometimes to compute some exemplar predicted probabilities to get a sense of the relationships:

      <<include=F>>=
      options(digits=3)
      @

      \begin{center}
        \begin{tabular}{r|llll}
          & \multicolumn{4}{c}{Height} \\
          & 60" & 64" & 68" & 72" \\
          \hline
          bisexual
            & \Sexpr{predict(model2, list(height=60, orientation="bisexual"), type="response")}
            & \Sexpr{predict(model2, list(height=64, orientation="bisexual"), type="response")}
            & \Sexpr{predict(model2, list(height=68, orientation="bisexual"), type="response")}
            & \Sexpr{predict(model2, list(height=72, orientation="bisexual"), type="response")}
            \\
          gay
            & \Sexpr{predict(model2, list(height=60, orientation="gay"), type="response")}
            & \Sexpr{predict(model2, list(height=64, orientation="gay"), type="response")}
            & \Sexpr{predict(model2, list(height=68, orientation="gay"), type="response")}
            & \Sexpr{predict(model2, list(height=72, orientation="gay"), type="response")}
            \\
          straight
            & \Sexpr{predict(model2, list(height=60, orientation="straight"), type="response")}
            & \Sexpr{predict(model2, list(height=64, orientation="straight"), type="response")}
            & \Sexpr{predict(model2, list(height=68, orientation="straight"), type="response")}
            & \Sexpr{predict(model2, list(height=72, orientation="straight"), type="response")}
            \\
        \end{tabular}
      \end{center}
    \end{frame}

    \begin{frame}
      We can also visualize this by plotting the three curves for straight (yellow), gay (green), and bisexual (blue) OkCupid users:
      <<echo=F>>=
      curve(exp(b0+b1*x+b2)/(1+exp(b0+b1*x+b2)), from=55, to=80,
          xlab='Height', ylab='Predicted P(male)', col="lightgreen", lwd=3)
      curve(exp(b0+b1*x+b3)/(1+exp(b0+b1*x+b3)), add=T, col="yellow", lwd=3)
      @
      Where will the curve for bisexual OkCupid users be?
    \end{frame}

    \begin{frame}
      We can also visualize this by plotting the three curves for straight (yellow), gay (green), and bisexual (blue) OkCupid users:
      <<echo=F>>=
      curve(exp(b0+b1*x)/(1+exp(b0+b1*x)), from=55, to=80,
          xlab='Height', ylab='Predicted P(male)', col="lightblue", lwd=3)
      curve(exp(b0+b1*x+b2)/(1+exp(b0+b1*x+b2)), add=T, col="lightgreen", lwd=3)
      curve(exp(b0+b1*x+b3)/(1+exp(b0+b1*x+b3)), add=T, col="yellow", lwd=3)
      @
    \end{frame}

    \section{Interactions in logistic regression}

    \begin{frame}{What would interactions do?}
      \begin{itemize}
        \item In linear regression, an interaction between two predictors $X_1$ and $X_2$ means that the \alert{slope} of $X_1$ will depend on the \alert{value} of $X_2$.
        \item In other words, there will be differently-sloped regression lines predicting $Y$ from $X_1$ depending on what the value of $X_2$ is.
      \end{itemize}
    \end{frame}

    \begin{frame}
      <<echo=F, fig.height=3.5>>=
      nba <- read.csv('../../data/nba.csv')
      model3 <- lm(PTS ~ N3PA * PCT3P, data=nba)

      plot(nba$N3PA, nba$PTS, pch=16, col='orange', xlab='Num 3-point shots attempted', ylab='Total points')
      a <- model3$coefficients["(Intercept)"] + mean(nba$PCT3P) * model3$coefficients["PCT3P"]
      b <- model3$coefficients["N3PA"] + model3$coefficients["N3PA:PCT3P"] * mean(nba$PCT3P)
      abline(a, b, col='yellow', lwd=4)
      text(29, b*29+a, "Average 3P% (35.4%)", col="yellow", pos=3, srt=b*43)

      a <- model3$coefficients["(Intercept)"] + (mean(nba$PCT3P) - 2*sd(nba$PCT3P)) * model3$coefficients["PCT3P"]
      b <- model3$coefficients["N3PA"] + model3$coefficients["N3PA:PCT3P"] * (mean(nba$PCT3P) - 2*sd(nba$PCT3P))
      abline(a, b, col='red', lwd=4)
      text(28.5, b*28.5+a, "2 SD below avg (31.7%)", col="red", pos=3, srt=b*45)

      a <- model3$coefficients["(Intercept)"] + (mean(nba$PCT3P) + 2*sd(nba$PCT3P)) * model3$coefficients["PCT3P"]
      b <- model3$coefficients["N3PA"] + model3$coefficients["N3PA:PCT3P"] * (mean(nba$PCT3P) + 2*sd(nba$PCT3P))
      abline(a, b, col='green', lwd=4)
      text(29, b*29+a, "2 SD above avg (39.2%)", col="green", pos=3, srt=b*40)

      breakeven <- -model3$coefficients["N3PA"] / model3$coefficients["N3PA:PCT3P"]
      a <- model3$coefficients["(Intercept)"] + breakeven * model3$coefficients["PCT3P"]
      b <- model3$coefficients["N3PA"] + model3$coefficients["N3PA:PCT3P"] * breakeven
      abline(a, b, col='pink', lwd=4)
      text(29, b*29+a, "26.5% 3P shooting", col="pink", pos=3, srt=b*45)
      @
    \end{frame}

    \begin{frame}{What would interactions do?}
      \begin{itemize}
        \item We can add interactions to logistic regression and the interpretation is the same: the effect of $X_1$ on the \alert{probability of being male} depends on the \alert{value} of $X_2$.
        \item Let's try this out with $X_1=\text{height}$ and $X_2=\text{orientation}$.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]
      \fontvsm
      <<>>=
      int.model <- glm(male ~ height * orientation, data=my.profiles, family=binomial)
      summary(int.model)
      @
    \end{frame}

    \begin{frame}
      The interaction model is:
      <<echo=F>>=
      b0 <- coef(int.model)['(Intercept)']
      b1 <- coef(int.model)['height']
      b2 <- coef(int.model)['orientationgay']
      b3 <- coef(int.model)['orientationstraight']
      b4 <- coef(int.model)['height:orientationgay']
      b5 <- coef(int.model)['height:orientationstraight']
      options(digits=2)
      @
      \begin{align*}
        \log\left(\frac{p}{1-p}\right) &=
          \Sexpr{b0} +
          \Sexpr{b1}\cdot\text{height}
          - \Sexpr{abs(b2)}\cdot\text{gay}
          - \Sexpr{abs(b3)}\cdot\text{straight} \\ & \qquad+
          \Sexpr{b4}\cdot\text{height}\cdot\text{gay} +
          \Sexpr{b5}\cdot\text{height}\cdot\text{straight}.
      \end{align*}
    \end{frame}

    \begin{frame}
      Let's graph the equation for gay (green), yellow (straight), and blue (bisexual) users:
      <<echo=F>>=
      curve(exp(b0+(b1+b4)*x+b2)/(1+exp(b0+(b1+b4)*x+b2)), from=55, to=80,
          xlab='Height', ylab='Predicted P(male)', col="lightgreen", lwd=3)
      curve(exp(b0+(b1+b5)*x+b3)/(1+exp(b0+(b1+b5)*x+b3)), add=T, col="yellow", lwd=3)
      curve(exp(b0+b1*x)/(1+exp(b0+b1*x)), add=T, col="lightblue", lwd=3)
      @
    \end{frame}

    \section{Hypothesis testing when there are 2+ predictors}

    \begin{frame}{Four kinds of hypotheses to test}
      \begin{enumerate}[<+->]
        \item \alert{Overall} null hypothesis: $\beta_1=\beta_2=\cdots=0$ (all of the slope coefficients are 0, the model has no predictive power at all)
        \item \alert{Quantitative variable} null hypothesis: $\beta_i=0$ (there is no relationship between gender and a particular predictor variable, holding constant the other predictors)
        \item \alert{Categorical variable} null hypothesis: $\beta=0$ for all dummy variables corresponding to this categorical variable (there is no relationship between gender and a particular predictor variable, holding constant the other predictors)
        \item \alert{Individual dummy variable coefficient} null hypothesis: $\beta_i=0$ (there is no difference in predicted probability of being male between this level and the reference level, holding constant other predictors)
      \end{enumerate}
    \end{frame}

    \begin{frame}{Likelihood ratio test}
      The \alert{likelihood ratio test} lets us test a null hypothesis of the form: Model A has no more predictive power than Model B.

      \bigskip

      We can use this to test null hypothesis that don't correspond to $p$-values that we can read off the regression output. (And remember that there's no $R^2$ or Adjusted $R^2$ in logistic regression to compare models!)
    \end{frame}

    \begin{frame}[fragile]{Example 1: Overall null hypothesis}
      We'll test the overall null hypothesis by comparing the model to a ``null model'' with no variables:
      \fontvsm
      <<message=F>>=
      library(lmtest)
      lrtest(model2)
      @
    \end{frame}

    \begin{frame}[fragile]{Example 2: Quantitative variable}
      We can test the significance of a quantitative variable (e.g., height) by reading the $p$-value for \texttt{height} off of the regression output:

      \fontvsm
      <<out.lines=11:18>>=
      summary(model2)
      @
    \end{frame}

    \begin{frame}[fragile]{Example 3: Categorical variable}
      We'll test the significance of a categorical variable by comparing the model with \texttt{orientation} to the model without it:
      \fontvsm
      <<message=F>>=
      model1 <- glm(male ~ height, data=my.profiles, family=binomial)
      model2 <- glm(male ~ height + orientation, data=my.profiles, family=binomial)
      lrtest(model1, model2)
      @
    \end{frame}

    \begin{frame}[fragile]{Example 4: Individual dummy variable}
      We can test the significance of the difference between two levels of a categorical variable (e.g. the difference between bisexual and straight) reading the $p$-value for \texttt{height:orientationstraight} off of the regression output:

      \fontvsm
      <<out.lines=11:18>>=
      summary(model2)
      @
    \end{frame}

    \section{Other applications of logistic regression}

    \begin{frame}{What else can we use logistic regression for?}
      \begin{itemize}
        \item \textbf{Finance:} Predicting which customers are most likely to default on a loan
        \item \textbf{Advertising:} Predicting when a customer will respond positively to an advertising campaign
        \item \textbf{Marketing:} Predicting when a customer will purchase a product or sign up for a service
      \end{itemize}
    \end{frame}
  \end{darkframes}
\end{document}
