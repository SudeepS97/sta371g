\documentclass{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{1, 0.894, 0.769}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.824,0.412,0.118}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{1,0.894,0.71}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.824,0.706,0.549}{#1}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.941,0.902,0.549}{#1}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.804,0.776,0.451}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.78,0.941,0.545}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{1,0.78,0.769}{#1}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{../371g-slides}
\usepackage{dcolumn}
\title{Multiple regression 2}
\subtitle{Lecture 8}
\author{STA 371G}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
  
  

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}
    \begin{frame}{The colleges data set}
      Today's data set is a sample of 1302 colleges with various factors about the colleges, including SAT scores, student/faculty ratios, tuition rates, acceptance rates, etc.
    \end{frame}

    \begin{frame}{Multiple regression assumptions}
      We need (the same!) four things to be true for statistical inference (i.e., hypothesis tests, $p$-values, confidence intervals) to work for multiple regression:
      \pause
      \begin{enumerate}
        \item The errors are independent.
        \item $Y$ is a linear function of the $X$'s (except for the errors).
        \item The errors are normally distributed.
        \item The variance of $Y$ is the same for any value of $X$ (``homoscedasticity'').
      \end{enumerate}
    \end{frame}

    \begin{frame}{Assumption 1: Independence of errors}
      Independence means that knowing the error (over-/under-prediction by the regression line) for one case doesn't tell you anything about the error for another case.

      \bigskip\pause

      Since each college is completely separate, there is no reason to think the errors are not independent.
    \end{frame}

    \begin{frame}{Multiple regression assumptions}
      \begin{enumerate}
        \item The errors are independent. \greencheckmark
        \item $Y$ is a linear function of the $X$'s (except for the errors).
        \item The errors are normally distributed.
        \item The variance of $Y$ is the same for any value of $X$ (``homoscedasticity'').
      \end{enumerate}
    \end{frame}

    \begin{frame}[fragile]{Assumption 2: Linearity}
      Look at the residual plot:
      \fontsm
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{plot}\hlstd{(}\hlkwd{predict}\hlstd{(model),} \hlkwd{residuals}\hlstd{(model),} \hlkwc{col}\hlstd{=}\hlstr{"green"}\hlstd{,}
\hlstd{+ }  \hlkwc{xlab}\hlstd{=}\hlstr{"Predicted values"}\hlstd{,} \hlkwc{ylab}\hlstd{=}\hlstr{"Residuals"}\hlstd{,} \hlkwc{pch}\hlstd{=}\hlnum{16}\hlstd{)}
\hlstd{> }\hlkwd{abline}\hlstd{(}\hlkwc{h}\hlstd{=}\hlnum{0}\hlstd{)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-2-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}{Multiple regression assumptions}
      \begin{enumerate}
        \item The errors are independent. \greencheckmark
        \item $Y$ is a linear function of the $X$'s (except for the errors). \greencheckmark
        \item The errors are normally distributed.
        \item The variance of $Y$ is the same for any value of $X$ (``homoscedasticity'').
      \end{enumerate}
    \end{frame}

    \begin{frame}[fragile]{Assumption 3: Normality of residuals}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{qqnorm}\hlstd{(}\hlkwd{residuals}\hlstd{(model))}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-3-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}{Multiple regression assumptions}
      \begin{enumerate}
        \item The errors are independent. \greencheckmark
        \item $Y$ is a linear function of the $X$'s (except for the errors). \greencheckmark
        \item The errors are normally distributed. \greencheckmark
        \item The variance of $Y$ is the same for any value of $X$ (``homoscedasticity'').
      \end{enumerate}
    \end{frame}

    \begin{frame}[fragile]{Assumption 4: Homoscedasticity}
      Look at the residual plot:
      \fontsm
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{plot}\hlstd{(}\hlkwd{predict}\hlstd{(model),} \hlkwd{residuals}\hlstd{(model),} \hlkwc{col}\hlstd{=}\hlstr{"green"}\hlstd{,}
\hlstd{+ }  \hlkwc{xlab}\hlstd{=}\hlstr{"Predicted values"}\hlstd{,} \hlkwc{ylab}\hlstd{=}\hlstr{"Residuals"}\hlstd{,} \hlkwc{pch}\hlstd{=}\hlnum{16}\hlstd{)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-4-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}{Multiple regression assumptions}
      \begin{enumerate}
        \item The errors are independent. \greencheckmark
        \item $Y$ is a linear function of the $X$'s (except for the errors). \greencheckmark
        \item The errors are normally distributed. \greencheckmark
        \item The variance of $Y$ is the same for any value of $X$ (``homoscedasticity''). \shrug[red]
      \end{enumerate}
    \end{frame}

    \begin{frame}
      \begin{center}
        Since one of the assumptions is not completely satisfied, we'll proceed with caution---i.e., take the $p$-values and confidence intervals with a grain of salt.  (We could try and fix the problem with a transformation, but let's just live with it for now.)
      \end{center}
    \end{frame}

    \begin{frame}{The overall null hypothesis for a regression model}
      The following are equivalent ways to express the overall null hypothesis with $k$ predictor variables:
      \begin{itemize}[<+->]
        \item $R^2=0$ (in the population)
        \item $\text{cor}(\hat Y,Y)=0$ (in the population)
        \item $\beta_1=\beta_2=\cdots=\beta_k=0$ (i.e., all coefficients are 0 except the intercept)
        \item The model has no predictive power
        \item Predictions from this model are no better than predicting $\overline Y$ for every case
      \end{itemize}
    \end{frame}

    \begin{frame}
      We should always test the overall null hypothesis for a model first. \alert{If we can't reject the overall null hypothesis, there's no reason to interpret the model further.}

      \bigskip\pause

      In this model, the overall $p$-value is very small, so we reject the overall null hypothesis and conclude that yes, this model does have some predictive power.
    \end{frame}

    \begin{frame}{Statistical vs practical significance}
      \begin{itemize}
        \item As in simple regression, once we determine that there is statistical significance, we want to then assess whether there is also practical significance.
        \item For the test of the overall null hypothesis, we look to the value of $R^2$ in the sample to assess practical significance.
      \end{itemize}
    \end{frame}

    \begin{frame}{Testing individual coefficients}
      \begin{itemize}[<+->]
        \item Next, we want to test $H_0 : \beta_i=0$ for each of the predictors $X_i$.
        \item This is equivalent to the null hypothesis that $X_i$ has no correlation with $Y$ once the other predictors are held constant.
        \item The test statistic for testing the null hypothesis $\beta_i = S$ follows a $t$-distribution with $n-k-1$ degrees of freedom: \[ t = \frac{\beta_i - S}{\text{SE}(\beta_i)} \]
        \item The regression output calculates the $p$-value for us for testing the null hypotheses $\beta_i = 0$.
        \item If we reject this null hypothesis for a coefficient, we say that $X_i$  is a (statistically) significant predictor of $Y$ in the model.
      \end{itemize}
    \end{frame}

    \begin{frame}{Testing individual coefficients}
      If a predictor is not statistically significant, we should:
      \begin{enumerate}[<+->]
        \item Interpret it as if it were zero.
        \item Remove it from the model, as it does not contribute to predicting $Y$ above and beyond the other predictors.
      \end{enumerate}
    \end{frame}

    % \begin{frame}{Comparing two models}
    %   <<echo=F, results="asis">>=
    %   model1 <- lm(Graduation.rate ~ Acceptance.rate, data=my.sample)
    %   model2 <- lm(Graduation.rate ~ Acceptance.rate + Average.combined.SAT, data=my.sample)
    %   stargazer(model1, model2,
    %     title="Models for predicting graduation rate",
    %     dep.var.caption="",
    %     report="vc*",
    %     dep.var.labels.include=F,
    %     column.labels=c("Model 1", "Model 2"),
    %     align=T,
    %     single.row=T,
    %     omit.stat=c("ser", "f", "n"),
    %     omit.table.layout=c("m")
    %   )
    %   @
    % \end{frame}

    \begin{frame}{Residual standard error}
      \begin{itemize}[<+->]
        \item Like with simple regression, the \alert{residual standard error} $s_e$ is approximately equal to the standard deviation of the residuals.
        \item Since one of the assumptions of regression is that the residuals are approximately normal, we can conclude that approximately 95\% of the residuals will be less than $\pm 2s_e$.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Confidence intervals for coefficents}
      Confidence intervals for the individual coefficients are found the same way as in simple regression, and interpreted the same way:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{confint}\hlstd{(model)}
\end{alltt}
\begin{verbatim}
                             2.5 %     97.5 %
(Intercept)          -16.905960009 0.25666876
Average.combined.SAT   0.051525739 0.07071843
In.state.tuition       0.001030476 0.00146680
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{Confidence intervals for predictions}
      \fontsm
      We can also put confidence intervals on our predictions for $Y$.

      \bigskip\pause

      A 95\% CI for the graduation rate at the University of California, Merced, which is not in the data set and has an average SAT score of 1100 and in-state tuition of \$11,502:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{predict}\hlstd{(model,} \hlkwd{list}\hlstd{(}\hlkwc{Average.combined.SAT}\hlstd{=}\hlnum{1100}\hlstd{,}
\hlstd{+ }                    \hlkwc{In.state.tuition}\hlstd{=}\hlnum{11502}\hlstd{),}
\hlstd{+ }               \hlkwc{interval}\hlstd{=}\hlstr{"prediction"}\hlstd{)}
\end{alltt}
\begin{verbatim}
       fit      lwr   upr
1 73.27148 46.24296 100.3
\end{verbatim}
\end{kframe}
\end{knitrout}
      

      \pause

      Our best guess for UC Merced is 73.27\%, with a 95\% CI of (46.24\%, 100.3\%). \pause (It turns out that the actual graduation rate at UC Merced is 64\%.)
    \end{frame}

    \begin{frame}[fragile]{Confidence intervals for predictions}
      \fontsm
      A 95\% CI for average graduation rate among all colleges with an average SAT score of 1100 and in-state tuition of \$11,502:
      
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{> }\hlkwd{predict}\hlstd{(model,} \hlkwd{list}\hlstd{(}\hlkwc{Average.combined.SAT}\hlstd{=}\hlnum{1100}\hlstd{,}
\hlstd{+ }                    \hlkwc{In.state.tuition}\hlstd{=}\hlnum{11502}\hlstd{),}
\hlstd{+ }               \hlkwc{interval}\hlstd{=}\hlstr{"confidence"}\hlstd{)}
\end{alltt}
\begin{verbatim}
      fit     lwr     upr
1 73.2715 71.8091 74.7338
\end{verbatim}
\end{kframe}
\end{knitrout}

      \bigskip\pause

      As with simple regression, our point estimate is the same, but the confidence interval is much narrower, because it's easier to estimate a mean than a prediction for a single new case.
    \end{frame}
  \end{darkframes}

\end{document}
