\documentclass{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{1, 0.894, 0.769}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.824,0.412,0.118}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{1,0.894,0.71}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.824,0.706,0.549}{#1}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.941,0.902,0.549}{#1}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.804,0.776,0.451}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.78,0.941,0.545}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{1,0.78,0.769}{#1}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{../371g-slides}
\title{Model building: problems and fixing them}
\subtitle{Lecture 14}
\author{STA 371G}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
  
  

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}
    \begin{frame}{Today's data set}
      We're going to look at a data set of newly hired managers:
      \begin{columns}[onlytextwidth]
        \column{.5\textwidth}
          \begin{itemize}
            \item Salary (response)
            \item Manager rating
            \item Years of experience
          \end{itemize}
        \column{.5\textwidth}
          \begin{itemize}
            \item Years since graduation
            \item Origin (internal or external hire)
          \end{itemize}
      \end{columns}
    \end{frame}

    \begin{frame}[fragile]{Data issues}
      \begin{center}
        Data scientists report that they spend \alert{70\% of their time on obtaining and cleaning the data}. Only 30\% is for statistical analysis.\bigskip \pause

        Never run a regression without exploring and cleaning the data first!
      \end{center}
    \end{frame}

    \begin{frame}
      \fullpagepicture{debbie-downer}
    \end{frame}

    \begin{frame}
      The most common issues:
      \tableofcontents
    \end{frame}

    \section{Outliers}

    \begin{frame}[fragile]{Exploring the data: Outliers}
      Boxplots are commonly used to detect outliers. Let's start by looking at the Salary column.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlkwd{boxplot}\hlstd{(manager}\hlopt{$}\hlstd{Salary,} \hlkwc{xlab}\hlstd{=}\hlstr{"Salary"}\hlstd{,} \hlkwc{horizontal}\hlstd{=T)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-2-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Outliers}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlkwd{subset}\hlstd{(manager, Salary} \hlopt{>} \hlnum{200}\hlstd{)}
\end{alltt}
\begin{verbatim}
    Salary MngrRating YearsExp YrsSinceGrad   Origin
146    511        6.1        2            2 Internal
\end{verbatim}
\begin{alltt}
\hlkwd{subset}\hlstd{(manager, Salary} \hlopt{<} \hlnum{0}\hlstd{)}
\end{alltt}
\begin{verbatim}
    Salary MngrRating YearsExp YrsSinceGrad   Origin
121    -66        5.7        1            2 Internal
\end{verbatim}
\end{kframe}
\end{knitrout}
      \pause

      We can deal with outliers in two ways.
      \begin{itemize}[<+->]
        \item If the result of \textbf{errors in the data}, we can try to correct or omit.
        \item If not, consider omitting, but report on them separately.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Outliers}
      Let's omit the outliers by creating a new data set \texttt{mclean} that consists of the subset of the data where the salary is between \$0 and \$200,000.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{mclean} \hlkwb{<-} \hlkwd{subset}\hlstd{(manager, Salary} \hlopt{>} \hlnum{0} \hlopt{&} \hlstd{Salary} \hlopt{<} \hlnum{200}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Outliers}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlkwd{boxplot}\hlstd{(mclean}\hlopt{$}\hlstd{YearsExp,} \hlkwc{xlab}\hlstd{=}\hlstr{"Years of Experience"}\hlstd{,}
  \hlkwc{horizontal}\hlstd{=T)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-5-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Outliers}
      99 must be a code for missing entry in the Years of Experience variable!

      \bigskip\pause

      Let's label all 99s as \texttt{NA} (``not available'' --- R's code for missing data). \pause
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{mclean}\hlopt{$}\hlstd{YearsExp[mclean}\hlopt{$}\hlstd{YearsExp} \hlopt{==} \hlnum{99}\hlstd{]} \hlkwb{<-} \hlnum{NA}
\end{alltt}
\end{kframe}
\end{knitrout}
    \end{frame}

    \section{Missing data}

    \begin{frame}[fragile]{Exploring the data: Missing entries}
      Let's see if we have other missing data. \pause
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{mclean[}\hlopt{!}\hlkwd{complete.cases}\hlstd{(mclean),]}
\end{alltt}
\begin{verbatim}
    Salary MngrRating YearsExp YrsSinceGrad   Origin
103     75         NA        8            8 Internal
110     81         NA        9            9 External
124     73        5.9       NA            7 External
154     49        8.0        1            1     <NA>
\end{verbatim}
\end{kframe}
\end{knitrout}
      \pause
      This isn't surprising---it is very common to have missing entries in your data.
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Missing entries}
      There are two ways of dealing with missing data:
      \begin{itemize}
        \item Omit the rows that have missing entries in it.
        \item Try to predict values to fill the missing entries.
      \end{itemize}
      Omitting data is the easiest, but often not the best way, \alert{because you lose all the other information available in the same row}.
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Missing entries}
      What should we replace the \texttt{NA}s in the Manager Rating and Years of Experience columns with? \pause \bigskip

      The simplest way would be to use the averages in the respective columns.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{mclean}\hlopt{$}\hlstd{MngrRating[}\hlkwd{is.na}\hlstd{(mclean}\hlopt{$}\hlstd{MngrRating)]} \hlkwb{<-}
  \hlkwd{mean}\hlstd{(mclean}\hlopt{$}\hlstd{MngrRating,} \hlkwc{na.rm}\hlstd{=T)}

\hlstd{mclean}\hlopt{$}\hlstd{YearsExp[}\hlkwd{is.na}\hlstd{(mclean}\hlopt{$}\hlstd{YearsExp)]} \hlkwb{<-}
  \hlkwd{mean}\hlstd{(mclean}\hlopt{$}\hlstd{YearsExp,} \hlkwc{na.rm}\hlstd{=T)}
\end{alltt}
\end{kframe}
\end{knitrout}

      \pause \bigskip
      A smarter and more advanced way is to predict the missing data from the other data (using regression!).
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Missing entries}
      What about the missing data for categorical variables? \pause
      Let's choose the easy way and omit them.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{mclean} \hlkwb{<-} \hlkwd{na.omit}\hlstd{(mclean)}
\end{alltt}
\end{kframe}
\end{knitrout}
      \pause
      This removes all the rows that contain missing entries (only the Origin column has missing entries in this case.) \pause \bigskip

      We could also predict the missing entries, or treat the missing entries as a seperate level (e.g. ``Unknown'').
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Missing entries}
      \begin{itemize}[<+->]
        \item While dealing with the missing data, we assume that the data is ``Missing Completely at Random'' (MCAR).
        \item If this assumption does not hold (e.g. if the missing data mostly belongs to external hires), the model will  be biased.
        \item Making predictions for missing data based on available data reinforces the existing relationships between variables, so impacts the standard error.
        \item If a lot of data is missing (e.g. more than 5\%) for a particular variable, you may have to discard the whole column.
      \end{itemize}
    \end{frame}

    \section{Multicollinearity}

    \begin{frame}[fragile]{Exploring the data: Multicollinearity}
      Multicollinearity exists whenever 2+ predictors in a regression model are moderately or highly correlated. \pause
      \begin{itemize}[<+->]
        \item If two predictors $X_1$ and $X_2$ are highly correlated, it is hard to estimate the effect of changing $X_1$ while keeping $X_2$ constant.
        \item This means we will have large standard errors, and large p-values, for $X_1$ and/or $X_2$.
        \item This \alert{does not} mean there isn't a relationship between $X_1$ and $Y$, or $X_2$ and $Y$ -- it just means we can't pin down that relationship, because of the correlation!
      \end{itemize}
      \pause
      Correlation between the response and the predictors is good, but correlation between the predictors is not!
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Multicollinearity}
      We want to avoid multicollinearity in our models! \pause
      \begin{itemize}[<+->]
        \item Any conclusions based on the p-values, coefficients, and confidence intervals of the highly correlated variables will be unreliable.
        \item These statistics will not be stable: adding new data or predictors to the model could drastically change them.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]%{Exploring the data: Multicollinearity}
      \fontsize{9}{9}\selectfont
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlkwd{pairs}\hlstd{(}\hlopt{~} \hlstd{MngrRating} \hlopt{+} \hlstd{YearsExp} \hlopt{+} \hlstd{YrsSinceGrad,} \hlkwc{data}\hlstd{=mclean)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-10-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]%{Exploring the data: Multicollinearity}
      \fontsize{8}{8}\selectfont
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{model} \hlkwb{<-} \hlkwd{lm}\hlstd{(Salary} \hlopt{~} \hlstd{MngrRating} \hlopt{+} \hlstd{YearsExp} \hlopt{+} \hlstd{YrsSinceGrad} \hlopt{+} \hlstd{Origin,}
           \hlkwc{data}\hlstd{=mclean)}
\hlkwd{summary}\hlstd{(model)}
\end{alltt}
\begin{verbatim}

Call:
lm(formula = Salary ~ MngrRating + YearsExp + YrsSinceGrad + 
    Origin, data = mclean)

Residuals:
     Min       1Q   Median       3Q      Max 
-19.7766  -4.2842  -0.2906   3.3266  28.2773 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)     54.1521     2.6071  20.771  < 2e-16 ***
MngrRating       4.5147     0.3997  11.296  < 2e-16 ***
YearsExp        -1.5262     1.3790  -1.107 0.270203    
YrsSinceGrad     0.7692     1.3833   0.556 0.578976    
OriginInternal  -4.7314     1.3878  -3.409 0.000838 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.838 on 149 degrees of freedom
Multiple R-squared:  0.6065,	Adjusted R-squared:  0.596 
F-statistic: 57.42 on 4 and 149 DF,  p-value: < 2.2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}
      \note{Point out the high standard errors}
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Multicollinearity}
      One way to see if two variables are collinear is to check the correlation between the two:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlkwd{cor}\hlstd{(mclean}\hlopt{$}\hlstd{YearsExp, mclean}\hlopt{$}\hlstd{YrsSinceGrad)}
\end{alltt}
\begin{verbatim}
[1] 0.9947616
\end{verbatim}
\end{kframe}
\end{knitrout}
      \pause
      Any correlation $\geq 0.95$ is definitely a problem, but smaller correlations could be problematic too.
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Multicollinearity}
      A better way to check multicollinearity is using Variance Inflation Factors (VIF).

      \begin{itemize}[<+->]
        \item The VIF is
          \[
            \text{VIF}(\beta_j) = \frac{1}{1 - R_j^2},
          \]
          where $R_j^2$ is the $R^2$ in a regression predicting $X$ variable $j$ from the other $X$ variables.
        \item $\text{VIF}(\beta_j)=0$ when $R_j^2=0$; i.e., the $j$th predictor variable is completely independent from the others.
        \item $\text{VIF}(\beta_j)$ increases as $R_j^2$ does, and is $\infty$ when there is perfect multicollinearity; i.e., when $X_j$ is perfectly predictable from the other $X$ variables.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Multicollinearity}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(car)}
\hlkwd{vif}\hlstd{(model)}
\end{alltt}
\begin{verbatim}
  MngrRating     YearsExp YrsSinceGrad       Origin 
    1.136002    95.954255    97.011260     1.540448 
\end{verbatim}
\end{kframe}
\end{knitrout}
      Predictors with $\text{VIF} > 5$ indicate multicollinearity.
      \pause\bigskip

      \alert{Remember:} Multicollinearity could exist between more than two predictors (this is why there are only $n-1$ dummy variables for a categorical variable with $n$ values).
    \end{frame}

    \begin{frame}
      \fullpagepicture{debbie-downer}
    \end{frame}

    \begin{frame}{Dealing with multicollinearity}
      There are two general strategies for dealing with multicollinearity:
      \begin{itemize}
        \item Drop a variable with a high VIF factor. (Just like we drop one of the dummy variables when putting a categorical variable in the model!)
        \item Combine the variables that correlate into a composite variable.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]%{Exploring the data: Multicollinearity}
      \fontsize{9}{9}\selectfont
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{model2} \hlkwb{<-} \hlkwd{lm}\hlstd{(Salary} \hlopt{~} \hlstd{MngrRating} \hlopt{+} \hlstd{YearsExp} \hlopt{+} \hlstd{Origin,} \hlkwc{data}\hlstd{=mclean)}
\hlkwd{summary}\hlstd{(model2)}
\end{alltt}
\begin{verbatim}

Call:
lm(formula = Salary ~ MngrRating + YearsExp + Origin, data = mclean)

Residuals:
     Min       1Q   Median       3Q      Max 
-19.8115  -4.3474  -0.3964   3.3358  28.1801 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)     54.1080     2.5999  20.812  < 2e-16 ***
MngrRating       4.5309     0.3977  11.394  < 2e-16 ***
YearsExp        -0.7651     0.1687  -4.534 1.18e-05 ***
OriginInternal  -4.6467     1.3762  -3.376 0.000935 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.823 on 150 degrees of freedom
Multiple R-squared:  0.6057,	Adjusted R-squared:  0.5978 
F-statistic: 76.82 on 3 and 150 DF,  p-value: < 2.2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \section{Highly influential points}

    \begin{frame}[fragile]{Finding highly influential points}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}
\input{/tmp/figures/unnamed-chunk-15-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{Outliers among the residuals}
      Let's look at row 157:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{manager[}\hlnum{157}\hlstd{,]}
\end{alltt}
\begin{verbatim}
    Salary MngrRating YearsExp YrsSinceGrad   Origin
157     95          4        1            1 Internal
\end{verbatim}
\end{kframe}
\end{knitrout}
      Someone with only 1 year of experience and a poor rating is hired as manager at \$95K! \pause \bigskip

      If you decide that this is an anomaly (e.g. the CEO's son was promoted!) that you don't want to include in your analysis, omit that row and report on it separately in your conclusions.
    \end{frame}

    \begin{frame}[fragile]{Influential cases}
      \begin{itemize}[<+->]
        \item The Residuals vs Leverage plot tells about \alert{influential cases}.
        \item A \textbf{high-leverage case} is one that has an unusual combination of predictor values.
        \item An \textbf{influential case} is a high-leverage case that also has a high residual: it could change your $\beta$ values significantly when excluded from your analysis, i.e., it does not follow the overall trend.
        \item Look for the cases on the upper/lower right corners (beyond the dashed curves).
      \end{itemize}
    \end{frame}

    \section{Handling nonlinearity}

    \begin{frame}[fragile]
      Let's look at the total number of votes the winning candidate for U.S. President has won since 1988:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(Votes} \hlopt{~} \hlstd{Year,} \hlkwc{data}\hlstd{=elections,} \hlkwc{pch}\hlstd{=}\hlnum{16}\hlstd{,} \hlkwc{col}\hlstd{=}\hlstr{"lightgreen"}\hlstd{)}
\end{alltt}
\end{kframe}
\input{/tmp/figures/unnamed-chunk-17-1.tex}

\end{knitrout}
      \pause Is a line a good fit for this data?
      \pause Is there any kind of transformation of either $X$ or $Y$ that can fix this nonlinearity?
    \end{frame}

    \begin{frame}[fragile]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{elections}\hlopt{$}\hlstd{Time} \hlkwb{<-} \hlstd{elections}\hlopt{$}\hlstd{Year} \hlopt{-} \hlnum{1988}
\hlstd{model1} \hlkwb{<-} \hlkwd{lm}\hlstd{(Votes} \hlopt{~} \hlstd{Time,} \hlkwc{data}\hlstd{=elections)}
\end{alltt}
\end{kframe}
\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}
\input{/tmp/figures/unnamed-chunk-19-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}
      \fullpagepicture{debbie-downer}
    \end{frame}

    \begin{frame}{Let's try to fit a polynomial!}
      \begin{itemize}[<+->]
        \item Think of a transformation of $X$ or $Y$ as fitting a curve to the data; for example, $X\to\log X$ fits a logarithmic curve to the data.
        \item A polynomial is a function of the form
          \[ f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \cdots + \beta_n x^n, \]
        for some $n$. We can fit a polynomial curve to our data by just adding the higher order $X^k$ terms as predictor variables to our regression model!
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{elections}\hlopt{$}\hlstd{TimeSquared} \hlkwb{<-} \hlstd{elections}\hlopt{$}\hlstd{Time}\hlopt{^}\hlnum{2}
\hlstd{model2} \hlkwb{<-} \hlkwd{lm}\hlstd{(Votes} \hlopt{~} \hlstd{Time} \hlopt{+} \hlstd{TimeSquared,} \hlkwc{data}\hlstd{=elections)}
\end{alltt}
\end{kframe}
\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}
\input{/tmp/figures/unnamed-chunk-21-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\begin{kframe}
\begin{alltt}
\hlstd{elections}\hlopt{$}\hlstd{TimeCubed} \hlkwb{<-} \hlstd{elections}\hlopt{$}\hlstd{Time}\hlopt{^}\hlnum{3}
\hlstd{model3} \hlkwb{<-} \hlkwd{lm}\hlstd{(Votes} \hlopt{~} \hlstd{Time} \hlopt{+} \hlstd{TimeSquared} \hlopt{+}
                       \hlstd{TimeCubed,} \hlkwc{data}\hlstd{=elections)}
\end{alltt}
\end{kframe}
\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}
\input{/tmp/figures/unnamed-chunk-23-1.tex}

\end{knitrout}
    \end{frame}

    \begin{frame}{Nonlinearity}
      \begin{itemize}
        \item Just like model selection with any other variable: use the statistical significance of the highest order term, and changes in $R^2$, to determine how many powers you should add.
        \item If you include a power $X^k$, you should also include $X, X^2, \ldots, X^{k-1}$, even if they are not statistically significant.
        \item Be particularly careful with extrapolation when using a polynomial model!
      \end{itemize}
    \end{frame}

  \end{darkframes}

  \end{document}
